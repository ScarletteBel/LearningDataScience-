{"cells":[{"cell_type":"markdown","metadata":{},"source":[" Assignment 2 NLP  ----   Scarlette Bello Barron    c0860234"]},{"cell_type":"markdown","metadata":{},"source":["#Sample of text: <br>\n","#â€œ         Jack and jill have made a delicious,       dish. Then they started to play some12 game! and jill attached the # [a] photo frame to the straight9 wall and swung on a sea-saw. She was very happy. After the game, they both went to central London to enjoy some fast food.â€œ"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The saple tet is:  â€œ         Jack and jill have made a delicious,       dish. Then they started to play some12 game! and jill attached the # [a] photo frame to the straight9 wall and swung on a sea-saw. She was very happy. After the game, they both went to central London to enjoy some fast food.â€œ\n","\n"]}],"source":["sample_text = 'â€œ         Jack and jill have made a delicious,       dish. Then they started to play some12 game! and jill attached the # [a] photo frame to the straight9 wall and swung on a sea-saw. She was very happy. After the game, they both went to central London to enjoy some fast food.â€œ'\n","print(\"The saple tet is: \", sample_text)\n","print()"]},{"cell_type":"markdown","metadata":{},"source":["1. Remove punctuation....<br>\n","lso numbers from words are removed"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import re "]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"]}],"source":["puncts_removed = re.sub(r'[^\\w\\s]', '', sample_text)\n","print()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Sample text with no punctuations neither numbers: âœ         Jack and jill have made a delicious       dish Then they started to play some game and jill attached the  a photo frame to the straight wall and swung on a seasaw She was very happy After the game they both went to central London to enjoy some fast foodâœ\n","\n"]}],"source":["world_range = r'[0-9]'\n","sample_text1= re.sub(world_range, '', puncts_removed)\n","print(\"Sample text with no punctuations neither numbers:\", sample_text1)\n","print()"]},{"cell_type":"markdown","metadata":{},"source":["2. Tokenize Sentence..."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Sample text tokenized by sentences:  ['Jack and jill have made a delicious       dish ', 'Then they started to play some game and jill attached the  a photo frame to the straight wall and swung on a seasaw ', 'She was very happy ', 'After the game they both went to central ', 'London to enjoy some fast foodâœ']\n","\n"]}],"source":["sample_text2 = re.findall('[A-Z][^A-Z]*', sample_text1)\n","print(\"Sample text tokenized by sentences: \", sample_text2)\n","print()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["3. Tokenize word..."]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcluster\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdownloader\u001b[39;00m \u001b[39mimport\u001b[39;00m download\n","File \u001b[1;32mc:\\Users\\scarb\\OneDrive\\Documentos\\BigData\\GitHub\\LearningDataScience-\\venv\\lib\\site-packages\\nltk\\__init__.py:180\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m \u001b[39mimport\u001b[39;00m cluster\n\u001b[1;32m--> 180\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdownloader\u001b[39;00m \u001b[39mimport\u001b[39;00m download, download_shell\n\u001b[0;32m    182\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtkinter\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\scarb\\OneDrive\\Documentos\\BigData\\GitHub\\LearningDataScience-\\venv\\lib\\site-packages\\nltk\\downloader.py:2479\u001b[0m\n\u001b[0;32m   2469\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   2472\u001b[0m \u001b[39m######################################################################\u001b[39;00m\n\u001b[0;32m   2473\u001b[0m \u001b[39m# Main:\u001b[39;00m\n\u001b[0;32m   2474\u001b[0m \u001b[39m######################################################################\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2477\u001b[0m \n\u001b[0;32m   2478\u001b[0m \u001b[39m# Aliases\u001b[39;00m\n\u001b[1;32m-> 2479\u001b[0m _downloader \u001b[39m=\u001b[39m Downloader()\n\u001b[0;32m   2480\u001b[0m download \u001b[39m=\u001b[39m _downloader\u001b[39m.\u001b[39mdownload\n\u001b[0;32m   2483\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdownload_shell\u001b[39m():\n","File \u001b[1;32mc:\\Users\\scarb\\OneDrive\\Documentos\\BigData\\GitHub\\LearningDataScience-\\venv\\lib\\site-packages\\nltk\\downloader.py:515\u001b[0m, in \u001b[0;36mDownloader.__init__\u001b[1;34m(self, server_index_url, download_dir)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[39m# decide where we're going to save things to.\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_dir \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdefault_download_dir()\n","File \u001b[1;32mc:\\Users\\scarb\\OneDrive\\Documentos\\BigData\\GitHub\\LearningDataScience-\\venv\\lib\\site-packages\\nltk\\downloader.py:1071\u001b[0m, in \u001b[0;36mDownloader.default_download_dir\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1067\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m \u001b[39m# Check if we have sufficient permissions to install in a\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m \u001b[39m# variety of system-wide locations.\u001b[39;00m\n\u001b[1;32m-> 1071\u001b[0m \u001b[39mfor\u001b[39;00m nltkdir \u001b[39min\u001b[39;00m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39mpath:\n\u001b[0;32m   1072\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(nltkdir) \u001b[39mand\u001b[39;00m nltk\u001b[39m.\u001b[39minternals\u001b[39m.\u001b[39mis_writable(nltkdir):\n\u001b[0;32m   1073\u001b[0m         \u001b[39mreturn\u001b[39;00m nltkdir\n","\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)"]}],"source":["import nltk\n","import cluster\n","from nltk.downloader import download\n","nltk.download('punkt')\n","nltk.download('popular')\n","nltk.download('shell')\n","import tkinter\n","import nltk.data\n","from nltk.tokenize import word_tokenize\n","sample_text3 = word_tokenize(sample_text1)\n","print(\"Sample text tokenized by words: \", sample_text3)\n","print()"]},{"cell_type":"markdown","metadata":{},"source":["4. Remove stopwords..."]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m stopwords\n\u001b[0;32m      3\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mstopwords\u001b[39m\u001b[39m'\u001b[39m)\n","File \u001b[1;32mc:\\Users\\scarb\\OneDrive\\Documentos\\BigData\\GitHub\\LearningDataScience-\\venv\\lib\\site-packages\\nltk\\__init__.py:180\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m \u001b[39mimport\u001b[39;00m cluster\n\u001b[1;32m--> 180\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdownloader\u001b[39;00m \u001b[39mimport\u001b[39;00m download, download_shell\n\u001b[0;32m    182\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtkinter\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\scarb\\OneDrive\\Documentos\\BigData\\GitHub\\LearningDataScience-\\venv\\lib\\site-packages\\nltk\\downloader.py:2479\u001b[0m\n\u001b[0;32m   2469\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   2472\u001b[0m \u001b[39m######################################################################\u001b[39;00m\n\u001b[0;32m   2473\u001b[0m \u001b[39m# Main:\u001b[39;00m\n\u001b[0;32m   2474\u001b[0m \u001b[39m######################################################################\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2477\u001b[0m \n\u001b[0;32m   2478\u001b[0m \u001b[39m# Aliases\u001b[39;00m\n\u001b[1;32m-> 2479\u001b[0m _downloader \u001b[39m=\u001b[39m Downloader()\n\u001b[0;32m   2480\u001b[0m download \u001b[39m=\u001b[39m _downloader\u001b[39m.\u001b[39mdownload\n\u001b[0;32m   2483\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdownload_shell\u001b[39m():\n","File \u001b[1;32mc:\\Users\\scarb\\OneDrive\\Documentos\\BigData\\GitHub\\LearningDataScience-\\venv\\lib\\site-packages\\nltk\\downloader.py:515\u001b[0m, in \u001b[0;36mDownloader.__init__\u001b[1;34m(self, server_index_url, download_dir)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[39m# decide where we're going to save things to.\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_dir \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdefault_download_dir()\n","File \u001b[1;32mc:\\Users\\scarb\\OneDrive\\Documentos\\BigData\\GitHub\\LearningDataScience-\\venv\\lib\\site-packages\\nltk\\downloader.py:1071\u001b[0m, in \u001b[0;36mDownloader.default_download_dir\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1067\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m \u001b[39m# Check if we have sufficient permissions to install in a\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m \u001b[39m# variety of system-wide locations.\u001b[39;00m\n\u001b[1;32m-> 1071\u001b[0m \u001b[39mfor\u001b[39;00m nltkdir \u001b[39min\u001b[39;00m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39mpath:\n\u001b[0;32m   1072\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(nltkdir) \u001b[39mand\u001b[39;00m nltk\u001b[39m.\u001b[39minternals\u001b[39m.\u001b[39mis_writable(nltkdir):\n\u001b[0;32m   1073\u001b[0m         \u001b[39mreturn\u001b[39;00m nltkdir\n","\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)"]}],"source":["import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'stopwords' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m stop_words \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(stopwords\u001b[39m.\u001b[39mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m      3\u001b[0m filtered_sentence \u001b[39m=\u001b[39m [w \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m sample_text3 \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m w\u001b[39m.\u001b[39mlower() \u001b[39min\u001b[39;00m stop_words]\n\u001b[0;32m      4\u001b[0m filtered_sentence \u001b[39m=\u001b[39m []\n","\u001b[1;31mNameError\u001b[0m: name 'stopwords' is not defined"]}],"source":["stop_words = set(stopwords.words('english'))\n","  \n","filtered_sentence = [w for w in sample_text3 if not w.lower() in stop_words]\n","filtered_sentence = []\n","  \n","for w in sample_text3:\n","    if w not in stop_words:\n","        filtered_sentence.append(w)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sample_text4 = filtered_sentence \n","print()\n","print(\"Sample text without stopwords: \",sample_text4)"]},{"cell_type":"markdown","metadata":{},"source":["5. Stemming text..."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from nltk.stem import PorterStemmer\n","ps = PorterStemmer()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sample_text5 = []"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for w in sample_text4:\n","    word_sec =  ps.stem(w)\n","    sample_text5.append(word_sec)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print()\n","print(\"Steammed sample text: \", sample_text5)"]},{"cell_type":"markdown","metadata":{},"source":["6. Lemmatize..."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from nltk.stem import WordNetLemmatizer\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lemmatizer = WordNetLemmatizer()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sample_text6 = []"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for w in sample_text4:\n","    word_lem =  lemmatizer.lemmatize(w)\n","    sample_text6.append(word_lem)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print()\n","print(\"Lemmatized sample text: \", sample_text6)"]},{"cell_type":"markdown","metadata":{},"source":["7. Count vectorization..."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cv = CountVectorizer()\n","cv_vector = cv.fit_transform(sample_text6)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print()\n","print(cv_vector.toarray())\n","print()"]},{"cell_type":"markdown","metadata":{},"source":["8. n-grams...<br>\n","-gram range (2,2)..."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from nltk import ngrams"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sentence = 'Jack and jill have made a delicious dish'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["n = 2\n","twograms = ngrams(sentence.split(), n)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["new_arr = []"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for elemento in twograms:\n","    new_string = f\"{elemento[0]}  {elemento[1]}\"\n","    new_arr.append(new_string)\n","print(new_arr)"]},{"cell_type":"markdown","metadata":{},"source":["-gram range (2,3)..."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sentence = 'Jack and jill have made a delicious dish'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["n = 3\n","threegrams = ngrams(sentence.split(), n)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["new_arr1 = []"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for elemento in threegrams:\n","    new_string1 = f\"{elemento[0]}  {elemento[1]} {elemento[2]}\"\n","    new_arr1.append(new_string1)\n","print(new_arr1)\n","print()"]},{"cell_type":"markdown","metadata":{},"source":[". TF-IDF ..."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["my_dict = {}\n","for word in sample_text3:\n","    if word in my_dict:  \n","        my_dict[word] += 1\n","    else:                \n","        my_dict[word] = 1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["total_words = len(sample_text3)\n","for word in my_dict:\n","    my_dict[word] = round((my_dict[word] / total_words),2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"These are the TF of the words: \", my_dict)\n","print()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vectorizer = TfidfVectorizer()\n","X = vectorizer.fit_transform(sample_text2)\n","vectorizer.get_feature_names_out()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"The Tdidf shape is: \", X.shape)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":2}
